Logger:
  logger_name: "inm706_test_transformer_adam_cross-entropy_gelu_relative_attention"
  project_name: "inm706_cwk"

model:
  hidden_size: 512
  max_seq_length: 3
  num_layers: 6
  expansion_factor: 4
  n_heads: 8
  activation: "GELU" # "ReLU", "GELU"
  norm_first: True
  relative_attention: True

train:
  epochs: 50
  batch_size: 64
  optimizer: "adam" # "adamW", "radam", "adam"
  loss_function: "cross-entropy" # "cross-entropy", "negative-log"
  label_smoothing: 0 # 0 for no label-smoothing
  learning_rate: 0.001
  use_gradient_clipping: False