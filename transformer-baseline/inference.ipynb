{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM706: Inference (Transformer)\n",
    "\n",
    "This is notebook is for inference of the models found in the [City-INM706](https://github.com/yasirbarlas/City-INM706) Github repository.\n",
    "\n",
    "The model used for inference here is our baseline model, and so the parameters reflect this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and models\n",
    "\n",
    "from models import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Training/Validation Dataset\n",
    "\n",
    "We need to import the dataset since it contains the dictionaries for our token-word mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2007723 sentence pairs\n",
      "Trimmed to 1802065 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "europarl-v7.fr-en.en 71012\n",
      "europarl-v7.fr-en.fr 96093\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TranslationDataset(lang1 = \"europarl-v7.fr-en.en\", lang2 = \"europarl-v7.fr-en.fr\", max_seq_len = 50, reverse = False, directory = \"../fr-en/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding_layer): Embedding(\n",
       "      (embed): Embedding(71012, 512)\n",
       "    )\n",
       "    (positional_encoder): PositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (word_embedding): Embedding(96093, 512)\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=512, out_features=96093, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Get model checkpoint\n",
    "model = torch.load(f\"checkpoints/checkpoint_transformer_latest.pth.tar\", map_location = device)\n",
    "\n",
    "# Encoder and Decoder, replace 128 with your hidden dimension size and other parameters\n",
    "# The parameters given are for our baseline model\n",
    "# Replace src_vocab_size and target_vocab_size with your number of words (for english and french respectively)\n",
    "transformer = Transformer(embed_dim = 512, src_vocab_size = train_dataset.input_lang.n_words,\n",
    "                        target_vocab_size = train_dataset.output_lang.n_words, seq_len = 50,\n",
    "                        num_layers = 6, expansion_factor = 4,\n",
    "                        n_heads = 8, activation = \"ReLU\", norm_first = False, relative_attention = False).to(device)\n",
    "\n",
    "transformer.load_state_dict(model[\"model_state_dict\"])\n",
    "\n",
    "# Ensure the Transformer is in evaluation mode\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2000 sentence pairs\n",
      "Trimmed to 1766 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "test2008-en.txt 5003\n",
      "test2008-fr.txt 5924\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset (change max_seq_len to your maximum sentence word length)\n",
    "# Choose any dataset by inserting their paths as below (or skip if not interested in model testing)\n",
    "test_dataset = TranslationDataset(lang1 = \"test2008-en.txt\", lang2 = \"test2008-fr.txt\", max_seq_len = 50, reverse = False, directory = \"../fr-en/\")\n",
    "\n",
    "# Dataloader for faster inference\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Dataset\n",
    "\n",
    "Filter the dataset for sentences containing only words found in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the filtered dataset\n",
    "def create_filtered_dataset(test_dataset, input_lang):\n",
    "    valid_pairs = []\n",
    "\n",
    "    for pair in test_dataset.pairs:\n",
    "        input_valid = all(word in input_lang.word2index for word in pair[0].split())\n",
    "        if input_valid:\n",
    "            valid_pairs.append(pair)\n",
    "\n",
    "    class FilteredTranslationDataset(TranslationDataset):\n",
    "        def __init__(self, input_lang, output_lang, pairs, max_seq_len = 50):\n",
    "            self.input_lang = input_lang\n",
    "            self.output_lang = output_lang\n",
    "            self.pairs = pairs\n",
    "            self.max_seq_len = max_seq_len\n",
    "            self.input_lang_voc = input_lang.word2index\n",
    "            self.output_lang_voc = output_lang.word2index\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.pairs)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            input_sentence = self.pairs[index][0]\n",
    "            output_sentence = self.pairs[index][1]\n",
    "            in_sentence, out_sentence = self.tokenize_pair((input_sentence, output_sentence))\n",
    "            input_ids = np.zeros(self.max_seq_len, dtype = np.int32)\n",
    "            target_ids = np.zeros(self.max_seq_len, dtype = np.int32)\n",
    "            input_ids[:len(in_sentence)] = in_sentence\n",
    "            target_ids[:len(out_sentence)] = out_sentence\n",
    "            return input_sentence, torch.tensor(input_ids, dtype = torch.long, device=device), torch.tensor(target_ids, dtype = torch.long, device = device)\n",
    "\n",
    "    return FilteredTranslationDataset(test_dataset.input_lang, test_dataset.output_lang, valid_pairs, test_dataset.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation\n",
    "\n",
    "Calculate BLEU and NIST for the test set imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barla\\OneDrive\\Documents\\AI Module Resources\\INM706\\Coursework\\Code\\transformer\\models.py:317: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.fc_out(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU: 0.059293001384640054\n",
      "Mean NIST: 1.1848307154946724\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = create_filtered_dataset(test_dataset, train_dataset.input_lang)\n",
    "test_dataloader = DataLoader(filtered_dataset, batch_size = 32)\n",
    "\n",
    "bleu_scores = []\n",
    "nist_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, src, target_tensor in test_dataloader:\n",
    "        output = transformer(src, target_tensor)\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu(output, target_tensor)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Calculate NIST score\n",
    "        nist_score = calculate_nist(output, target_tensor)\n",
    "        nist_scores.append(nist_score)\n",
    "\n",
    "print(\"Mean BLEU:\", np.mean(bleu_scores))\n",
    "print(\"Mean NIST:\", np.mean(nist_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Sentence Evaluation\n",
    "\n",
    "One may wish to enter their own sentence (one that contains the words used during training) for inference. You should use lowercase without any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for inference\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \") if word in lang.word2index]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    EOS_token = 1\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 34]) torch.Size([1, 1])\n",
      "SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n"
     ]
    }
   ],
   "source": [
    "sentence = \"mr president ladies and gentlemen in his policy statement yesterday mr prodi the president of the commission said that whoever weakened any institution of the european union weakened the union as a whole\"\n",
    "\n",
    "# Let 0 be SOS token and 1 be EOS token\n",
    "src = tensorFromSentence(train_dataset.input_lang, sentence)\n",
    "target = torch.tensor([[0]], device = device)\n",
    "\n",
    "print(src.shape, target.shape)\n",
    "\n",
    "# Translate the input sentence\n",
    "output_indices = transformer.decode(src, target)\n",
    "\n",
    "decoded_words = []\n",
    "\n",
    "for i in output_indices:\n",
    "    decoded_words.append(train_dataset.output_lang.index2word[i])\n",
    "\n",
    "output_sentence = \" \".join(decoded_words)\n",
    "\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
